{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Práctica de Web Scraping\n",
    "\n",
    "Se va a considerar una de las páginas vista en la teoría que contiene fotos de trenes:https://www.vialibre-ffe.com/multi_foto.asp?cs=mult. En esta página aparece un menú con varios grupos de fotos de trenes. A su vez, cada grupo contiene un conjunto de subgrupos de fotos. Por ejemplo, el grupo __Renfe operadora__ se encuentra en la página https://www.vialibre-ffe.com/multi_ind_fotos.asp?cat=mu01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='Captura1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dentro de este grupo por ejemplo se encuentra el subgrupo __Serie120 en Irún-Hendaya__ en la página https://www.vialibre-ffe.com/multi_galeria.asp?gal=524. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='Captura2.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En dicho subgrupo se encuentran las imágenes de dicho subgrupo. Por ejemplo, a continuación, se muestra una de las imágenes del subgrupo __Serie120 en Irún-Hendaya__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='Captura1.png')\n",
    "#for i in range(1,4):\n",
    "#    Image(filename='Captura'+str(i)+'.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las imágenes contenidas en dicha página se pueden encontrar en las etiquetas __&lt;li> &lt;img ...__ que son hijas de la etiqueta __&lt;ul class=\"pgwSlideshow\">__. Por ejemplo en la página de ejemplo, la primera imagen de dicha página es:\n",
    "\n",
    "   __&lt;li>&lt;img src=\"multimedia/galerias/IRUN120/2Alvia_120.jpg\" alt=\"\" data-description=\"\">&lt;/li>__\n",
    "   \n",
    "A continuación, se puede ver el contenido html de esa página."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url=\"https://www.vialibre-ffe.com/multi_galeria.asp?gal=524\"\n",
    "r = requests.get(url)\n",
    "html = r.text\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pide:\n",
    "\n",
    " 1. Crear un programa en __Python__ que muestre un menú con los grupos de fotos, y que pregunte al usuario que grupo de fotos quiere visitar. Se puede asociar a cada grupo un número, y solicitar al usuario que introduzca por teclado el número del grupo. A continuación, se mostrará los subgrupos de ese grupo, y se le preguntará nuevamente al usuario que subgrupo de fotos quiere procesar. Igual que antes, se puede asociar a cada subgrupo un número, y que introduzca por teclado el número del subgrupo. Como resultado se creará un directorio en el disco local para el subgrupo elegido y en el directorio se bajarán las imágenes de los trenes. Se imprimirá por pantalla las urls de las imágenes que se están bajando. Toda la información utilizada en el programa, debe ser extraida de las páginas consideradas usando BeautifulSoup[6 puntos]\n",
    " \n",
    " 2. Crear un minibuscador en __Python__ que pregunte al usuario un conjunto de palabras clave, y  recorra el sitio web buscando todos los subgrupos en cuyo título aparezca alguna de las palabras claves. Como resultado debe mostrar los subgrupos encontrados, listando el nombre y la url de cada subgrupo [4 puntos]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Normas de entrega\n",
    "\n",
    "* Fecha tope de entrega: 18/11/2017\n",
    "* La entrega se realizará subiendo al campus virtual un notebook de Jupyter con la solución. El archivo tendrá como nombre WebScraping_GrupoX donde X será el número de grupo correspondiente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** EJERCICIO 1 ****\n",
      "\n",
      "------------------------------------Menú Principal:------------------------------------\n",
      "1. Renfe Operadora\n",
      "2. Ferrocarriles autonómicos\n",
      "3. Metropolitanos\n",
      "4. Tranvías\n",
      "5. Otros operadores\n",
      "6. Infraestructuras\n",
      "7. Instalaciones\n",
      "8. Estaciones\n",
      "9. Internacional\n",
      "10. Históricas\n",
      "11. Museos y trenes\n",
      "12. Varios\n",
      "Elige una opcion: 3\n",
      "------------------------------------Menú para Metropolitanos:------------------------------------\n",
      "1. Metro de Granada\n",
      "2. Metro Málaga\n",
      "3. Serie 300 de Metro de Barcelona\n",
      "4. Unidades del Metro de Málaga\n",
      "5. Coche serie 100 de TMB\n",
      "6. Los trenes de cercanías de CAF para Esmirna (Turquía)\n",
      "7. Metro Valencia\n",
      "8. Diseño coches línea 9 del Metro de Barcelona\n",
      "9. Metro Automatico de Laussane (Suiza)\n",
      "10. Metro de Pekin\n",
      "11. Metro de Sevilla\n",
      "12. Libro \"Metro\" de Alberto Schommer\n",
      "13. Metro Barcelona L9\n",
      "14. Metro de El Cairo\n",
      "15. Los nuevos trenes adaptados de Metro de Madrid\n",
      "16. Nueva estación de Guinardó/HospitalSant Pau\n",
      "17. Metro de Palma de Mallorca\n",
      "18. El Metro de Tokio\n",
      "19. Subte de Buenos Aires\n",
      "20. Metro de Bilbao\n",
      "21. 90 Años de Metro Madrid\n",
      "22. Metro de Barcelona\n",
      "23. Metro de Barcelona\n",
      "24. Metrosur de Madrid\n",
      "Elige una opcion: 3\n",
      "------------------------------------URLs de las imágenes:------------------------------------\n",
      "https://www.vialibre-ffe.com/multimedia/galerias/bcn/tmb_MA_100415_15_2.jpg\n",
      "https://www.vialibre-ffe.com/multimedia/galerias/bcn/tmb_MA_100415_15_300.jpg\n",
      "https://www.vialibre-ffe.com/multimedia/galerias/bcn/tmb_MA_100415_15_1924.jpg\n",
      "https://www.vialibre-ffe.com/multimedia/galerias/bcn/tmb_MA_100415_73.jpg\n",
      "https://www.vialibre-ffe.com/multimedia/galerias/bcn/tmb_MA_100415_75.jpg\n",
      "https://www.vialibre-ffe.com/multimedia/galerias/bcn/tmb_MA_100415_97.jpg\n",
      "https://www.vialibre-ffe.com/multimedia/galerias/bcn/tmb_MA_100415_1924.jpg\n",
      "------------------------------------Imágenes descargadas------------------------------------\n",
      "\n",
      "\n",
      "**** EJERCICIO 2 ****\n",
      "\n",
      "Introduzca las palabras clave a buscar (separadas por espacios): Serie120\n",
      "\n",
      "Serie120 en Irún-Hendaya : https://www.vialibre-ffe.com/multi_galeria.asp?gal=524\n",
      "\n",
      "Se han encontrado 1 resultado(s)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib\n",
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# **** EJERCICIO 1 ****\n",
    "def explorarFerrocarriles():\n",
    "    print('**** EJERCICIO 1 ****')\n",
    "    print()\n",
    "    #URL base de la pagina web\n",
    "    url=\"https://www.vialibre-ffe.com\"\n",
    "    #HTML de la pagina en la que empezamos a trabajar\n",
    "    html=urllib.request.urlopen(\"https://www.vialibre-ffe.com/multi_foto.asp?cs=mult\")\n",
    "    soup=BeautifulSoup(html.read(),'html.parser')\n",
    "    primero=soup.find_all(\"a\") # Buscamos todos los links\n",
    "    # Regex para reconocer las URLs que nos interesan\n",
    "    patron=re.compile(r\"\\/multi\\_ind\\_fotos\\.asp\\?cat=mu.*\")\n",
    "    # Lista que contendra los nombres de los links\n",
    "    listaNombres=[]\n",
    "    # Lista que contendra las URLs\n",
    "    listaURL=[]\n",
    "    # Parsea el HTML de la primera pagina para buscar las URLs que coinciden\n",
    "    # con el patron; despues los guarda en listas\n",
    "    for etiqueta in primero:\n",
    "        nomb = etiqueta.contents\n",
    "        link = etiqueta.get('href',None)\n",
    "        matches=patron.search(link)\n",
    "        # Si el patron coincide\n",
    "        if matches != None:\n",
    "            listaNombres.append(nomb[0])\n",
    "            listaURL.append(link)\n",
    "    # Muestra por pantalla las opciones disponibles\n",
    "    print(\"------------------------------------Menú Principal:------------------------------------\")\n",
    "    con = 0\n",
    "    for nomb in listaNombres:\n",
    "        con = con + 1\n",
    "        print(str(con) + \". \" + nomb)\n",
    "    # Pide la eleccion del usuario por pantalla y comprueba que la opcion es valida\n",
    "    opt = input(\"Elige una opcion: \")\n",
    "    try:\n",
    "        opt = int(opt)\n",
    "    except:\n",
    "        opt = 0\n",
    "        print(\"Por favor, introduce solo números\")\n",
    "    while (opt<1 or opt >len(listaNombres)):\n",
    "        print(\"No existe esa opción. Elige otra.\")\n",
    "        print()\n",
    "        opt = input(\"Elige una opcion: \")\n",
    "        try:\n",
    "            opt = int(opt)\n",
    "        except:\n",
    "            opt = 0\n",
    "            print(\"Por favor, introduce solo números\")\n",
    "    #Creamos una carpeta con el nombre de la opcion introducida y me meto en ella\n",
    "    #si ya existe esa carpeta no la creo solo me meto en ella\n",
    "    if os.path.exists(str(listaNombres[int(opt)-1])) is False:\n",
    "        os.makedirs(str(listaNombres[int(opt)-1]))\n",
    "    os.chdir(str(listaNombres[int(opt)-1]))\n",
    "    # Elige el link sobre el que trabajar de la lista anterior\n",
    "    link = url+listaURL[int(opt)-1]\n",
    "    # Se repite de nuevo el proceso para el nuevo link, que es\n",
    "    # un submenu de opciones.\n",
    "    html=urllib.request.urlopen(link)\n",
    "    soup=BeautifulSoup(html.read(),'html.parser')\n",
    "    segundo=soup.find_all(\"a\")\n",
    "    # El patron para el submenu es diferente; asi como las listas.\n",
    "    patron2=re.compile(r\"multi\\_galeria\\.asp\\?gal=.*\")\n",
    "    listaURL2 = []\n",
    "    listaNombres2 = []\n",
    "    # Añade a la lista los links que coinciden con el patron buscado y\n",
    "    # no esten ya en la lista (algunos links aparecian duplicados)\n",
    "    for etiqueta in segundo:\n",
    "        link = etiqueta.get('href',None)\n",
    "        matches=patron2.search(link)\n",
    "        if matches != None and link not in listaURL2:\n",
    "            listaURL2.append(link)\n",
    "            listaNombres2.append(etiqueta.contents[1].get('alt',None))\n",
    "    # Muestra las nuevas opciones por pantalla\n",
    "    print(\"------------------------------------Menú para \"+str(listaNombres[int(opt)-1])+\":------------------------------------\")\n",
    "    con = 0\n",
    "    for nomb in listaNombres2:\n",
    "        con = con + 1\n",
    "        print(str(con) + \". \" + nomb)\n",
    "    # Pide la eleccion del usuario por pantalla y comprueba que la opcion es valida\n",
    "    opt = input(\"Elige una opcion: \")\n",
    "    try:\n",
    "        opt = int(opt)\n",
    "    except:\n",
    "        opt = 0\n",
    "        print(\"Por favor, introduce solo números\")\n",
    "    while (opt<1 or opt >len(listaNombres2)):\n",
    "        print(\"No existe esa opción. Elige otra.\")\n",
    "        print()\n",
    "        opt = input(\"Elige una opcion: \")\n",
    "        try:\n",
    "            opt = int(opt)\n",
    "        except:\n",
    "            opt = 0\n",
    "            print(\"Por favor, introduce solo números\")\n",
    "    #Creo otra carpeta para indicar la otra opcion, en esata será donde guarde las fotos\n",
    "    #Si ya existe esa carpeta solo me meto en ella, pero sin crearla\n",
    "    if os.path.exists(str(listaNombres2[int(opt)-1])) is False:\n",
    "        os.makedirs(str(listaNombres2[int(opt)-1]))\n",
    "    os.chdir(str(listaNombres2[int(opt)-1]))\n",
    "    \n",
    "    #Tengo el link donde estan las fotos que quiero descargar\n",
    "    link = url+'/'+listaURL2[int(opt)-1]\n",
    "    #Cogo el html de esa pagina, que es donde estan las imagenes que quiero\n",
    "    html=urllib.request.urlopen(link).read()\n",
    "    soup=BeautifulSoup(html, 'html.parser')\n",
    "    #Guardo en una lista las etiquetas ul de la pagina\n",
    "    ul=[]\n",
    "    ul=soup.find_all(\"ul\")\n",
    "    #Consigo la ul <ul class=\"pgwSlideshow\">,que es la ultima que aparece en el html\n",
    "    li =ul[len(ul)-1]\n",
    "    #Guardo en una lista las etiquetas li que aparecen (que cada una de ellas es una imagen)\n",
    "    lo=li.find_all(\"li\")\n",
    "    j=1 #contador para ir nombrando a las imagenes una vez las guardemos\n",
    "    print(\"------------------------------------URLs de las imágenes:------------------------------------\")\n",
    "    #Recorro cada imagen\n",
    "    for imagen in lo:\n",
    "        #Consigo en que posicion esta el link de la imagen\n",
    "        posInicio=str(imagen).find('multimedia')\n",
    "        posFinal=str(imagen).find('\"/>')\n",
    "        imagenUrl=\"\"\n",
    "        #Recorro eso para obtener la direccion de la foto\n",
    "        while (posInicio!=posFinal):\n",
    "            imagenUrl=imagenUrl+str(imagen)[posInicio]\n",
    "            posInicio=posInicio+1\n",
    "        #Url de cada imagen\n",
    "        urlEntera=url+'/'+imagenUrl\n",
    "        print(urlEntera)#url entera a mostrar \n",
    "        #Descargo la foto\n",
    "        try:\n",
    "            img = urllib.request.urlopen(urlEntera)\n",
    "            manf=open(\"imagen\"+str(j)+\".jpg\",\"wb\")\n",
    "            while True:\n",
    "                info=img.read(100000)\n",
    "                if len(info)<1:\n",
    "                    break\n",
    "                manf.write(info)\n",
    "            manf.close()\n",
    "            j=j+1\n",
    "        except:\n",
    "            error=0\n",
    "    print(\"------------------------------------Imágenes descargadas------------------------------------\")\n",
    "    #Vuelvo a la carpeta raiz donde estaba\n",
    "    os.chdir(\"../../\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "# **** EJERCICIO 2 ****\n",
    "def miniBuscador():\n",
    "    print('**** EJERCICIO 2 ****')\n",
    "    print()\n",
    "    #URL base de la pagina web\n",
    "    urlBase = \"https://www.vialibre-ffe.com\"\n",
    "    #HTML de la pagina en la que empezamos a trabajar\n",
    "    html = urllib.request.urlopen(\"https://www.vialibre-ffe.com/multi_foto.asp?cs=mult\")\n",
    "    soup = BeautifulSoup(html.read(),'html.parser')\n",
    "    #Obtenemos las etiquetas donde están los enlaces a los grupos\n",
    "    etiquetasGrupos = soup.find_all(href=re.compile(\"\\/multi\\_ind\\_fotos\\.asp\\?cat=mu.*\"))\n",
    "  \n",
    "    #Pedimos al usuario las palabras que quiere buscar\n",
    "    entrada = []\n",
    "    while(len(entrada) == 0):\n",
    "        entrada = input(\"Introduzca las palabras clave a buscar (separadas por espacios): \")\n",
    "        #Las separamos\n",
    "        entrada = entrada.strip()\n",
    "        palabrasClave = entrada.split()\n",
    "        if (len(entrada) == 0):\n",
    "            print('*Por favor, introduzca al menos una palabra para poder realizar la búsqueda*')\n",
    "        print()\n",
    "    \n",
    "    numResultados = 0;\n",
    "    #Por cada una de las palabras introducidas\n",
    "    for palabra in palabrasClave:\n",
    "        #Buscamos en cada uno de los grupos de la web\n",
    "        for grupo in etiquetasGrupos:\n",
    "            html = urllib.request.urlopen(urlBase+grupo.get(\"href\",None))\n",
    "            soup = BeautifulSoup(html.read(),'html.parser')\n",
    "            #Cogemos todas las etiquetas <p>\n",
    "            subgrupos = soup.find_all('p')\n",
    "            for parrafos in subgrupos:\n",
    "                #Y dentro de ellas nos quedamos con las que tienen como hija una\n",
    "                #etiqueta <a>, ya que solamente coincide con esta característa los\n",
    "                #subgrupos que buscamos\n",
    "                subgrupo = parrafos.find('a')\n",
    "                #Si el título del subfrupo contiene la palabra buscada\n",
    "                if (subgrupo != None and palabra.lower() in subgrupo.string.lower()):\n",
    "                    #Mostramos el nombre y la url\n",
    "                    print(subgrupo.string, ':', urlBase+'/'+subgrupo.get(\"href\",None))\n",
    "                    numResultados += 1       \n",
    "    print()\n",
    "    print('Se han encontrado', numResultados, 'resultado(s)')\n",
    "    \n",
    "    \n",
    "    \n",
    "# ****LLAMADAS A LAS FUNCIONES****\n",
    "\n",
    "#Ejercicio 1\n",
    "explorarFerrocarriles()  \n",
    "#Ejercicio 2\n",
    "miniBuscador()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
